{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cac2824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import urllib\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd67f6d",
   "metadata": {},
   "source": [
    "# Weather Data Preprocessor\n",
    "\n",
    "Adopted from `GHCN_data_preprocessing.ipynb` at https://drive.google.com/file/d/1-ROib2DJIDWg1njvard4sJS8vGc7GrVT/view?usp=share_link.\n",
    "\n",
    "Although `GHCN_data_preprocessing.ipynb` was helpful, using `readlines()` was very slow on such a large CSV file. When running locally, I found it would take around 3 days to parse through a single year of data. Therefore, since my computer was capable (32GB of RAM), I decided to load in the entire CSV into a pandas dataframe and perform the same processing steps.\n",
    "\n",
    "# Download the data\n",
    "\n",
    "If `wget` is not installed on your machine, you may want to try `curl URL > file.txt` or the `urllib` package in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e6be211f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Windows [Version 10.0.22621.819]\n",
      "(c) Microsoft Corporation. All rights reserved.\n",
      "\n",
      "C:\\Data\\berkeley_classes\\fa22\\data200-grad-project>mkdir data_ghcn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A subdirectory or file data_ghcn already exists.\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Data\\berkeley_classes\\fa22\\data200-grad-project>cd data_ghcn\n",
      "\n",
      "C:\\Data\\berkeley_classes\\fa22\\data200-grad-project\\data_ghcn>curl https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt > ghcnd-stations.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 10.1M  100 10.1M    0     0  3127k      0  0:00:03  0:00:03 --:--:-- 3128k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Data\\berkeley_classes\\fa22\\data200-grad-project\\data_ghcn>curl https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2010.csv.gz > 2010.csv.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  171M  100  171M    0     0  7223k      0  0:00:24  0:00:24 --:--:-- 20.4M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "C:\\Data\\berkeley_classes\\fa22\\data200-grad-project\\data_ghcn>    \n",
      "C:\\Data\\berkeley_classes\\fa22\\data200-grad-project\\data_ghcn>"
     ]
    }
   ],
   "source": [
    "%%cmd\n",
    "mkdir data_ghcn\n",
    "cd data_ghcn\n",
    "curl https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/ghcnd-stations.txt > ghcnd-stations.txt\n",
    "curl https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/by_year/2010.csv.gz > 2010.csv.gz\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a3e6a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, must manually download the years from above by\n",
    "# changing the associated code (specifically, the years)\n",
    "years_to_process = ['2010', '2011', '2012', '2013', '2014', '2015', '2016']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "df8f0af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip the data\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "for curr_year in years_to_process:\n",
    "    with gzip.open('./data_ghcn/{}.csv.gz'.format(curr_year), 'rb') as f_in:\n",
    "        with open('./data_ghcn/{}.csv'.format(curr_year), 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88139b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abatics\\AppData\\Local\\Temp\\ipykernel_14804\\2670145683.py:8: FutureWarning: In a future version of pandas all arguments of read_csv except for the argument 'filepath_or_buffer' will be keyword-only.\n",
      "  stations = get_stations()\n",
      "C:\\Users\\Abatics\\AppData\\Local\\Temp\\ipykernel_14804\\2670145683.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  df = pd.read_csv(filename, '/t', header=None)\n"
     ]
    }
   ],
   "source": [
    "# get the station dataframe\n",
    "def get_stations(filename='data_ghcn/ghcnd-stations.txt'):\n",
    "    df = pd.read_csv(filename, '/t', header=None)\n",
    "    df = df[0].str.split(expand=True)[[0, 1, 2, 3]]\n",
    "    df.columns = ['Station', 'Latitude', 'Longitude', 'Elevation']\n",
    "    return df\n",
    "\n",
    "stations = get_stations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "162976da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading ./data_ghcn/2010.csv\n",
      "Finished Pivot Table for 2010\n",
      "Finished merge with station and type conversion for 2010\n",
      "Started save for 2010\n",
      "Finished saving CSV to ./data_ghcn/daily_global_weather_2010.csv\n",
      "Finished processing for 2010\n",
      "\n",
      "Finished reading ./data_ghcn/2011.csv\n",
      "Finished Pivot Table for 2011\n",
      "Finished merge with station and type conversion for 2011\n",
      "Started save for 2011\n",
      "Finished saving CSV to ./data_ghcn/daily_global_weather_2011.csv\n",
      "Finished processing for 2011\n",
      "\n",
      "Finished reading ./data_ghcn/2012.csv\n",
      "Finished Pivot Table for 2012\n",
      "Finished merge with station and type conversion for 2012\n",
      "Started save for 2012\n",
      "Finished saving CSV to ./data_ghcn/daily_global_weather_2012.csv\n",
      "Finished processing for 2012\n",
      "\n",
      "Finished reading ./data_ghcn/2013.csv\n",
      "Finished Pivot Table for 2013\n",
      "Finished merge with station and type conversion for 2013\n",
      "Started save for 2013\n",
      "Finished saving CSV to ./data_ghcn/daily_global_weather_2013.csv\n",
      "Finished processing for 2013\n",
      "\n",
      "Finished reading ./data_ghcn/2014.csv\n",
      "Finished Pivot Table for 2014\n",
      "Finished merge with station and type conversion for 2014\n",
      "Started save for 2014\n",
      "Finished saving CSV to ./data_ghcn/daily_global_weather_2014.csv\n",
      "Finished processing for 2014\n",
      "\n",
      "Finished reading ./data_ghcn/2015.csv\n",
      "Finished Pivot Table for 2015\n",
      "Finished merge with station and type conversion for 2015\n",
      "Started save for 2015\n",
      "Finished saving CSV to ./data_ghcn/daily_global_weather_2015.csv\n",
      "Finished processing for 2015\n",
      "\n",
      "Finished reading ./data_ghcn/2016.csv\n",
      "Finished Pivot Table for 2016\n",
      "Finished merge with station and type conversion for 2016\n",
      "Started save for 2016\n",
      "Finished saving CSV to ./data_ghcn/daily_global_weather_2016.csv\n",
      "Finished processing for 2016\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for curr_year in years_to_process:\n",
    "    # read in the weather data csv file\n",
    "    df = pd.read_csv(\"./data_ghcn/{}.csv\".format(curr_year), header=None)\\\n",
    "                     .drop(columns=[4, 5, 6, 7])\n",
    "    print('Finished reading ./data_ghcn/{}.csv'.format(curr_year))\n",
    "\n",
    "    # rename the columns\n",
    "    df.columns = ['Station', 'Date', 'Element', 'Value']\n",
    "\n",
    "    # get only rows with TAVG and PRCP\n",
    "    prcp_tavg = df[(df['Element']=='TAVG') | (df['Element']=='PRCP')]\n",
    "\n",
    "    # drop rows that do not have associated station names\n",
    "    prcp_tavg_valid = prcp_tavg[prcp_tavg['Station']\\\n",
    "                                .isin(stations['Station'])]\n",
    "\n",
    "    # pivot_table to get TAVG and PRCP as columns\n",
    "    prcp_tavg_pivoted = prcp_tavg_valid.pivot_table(index=['Date', 'Station'],\n",
    "                                values='Value',\n",
    "                                columns='Element')\\\n",
    "                                .dropna()\\\n",
    "                                .rename_axis(None, axis=1)\\\n",
    "                                .reset_index()\n",
    "    print('Finished Pivot Table for {}'.format(curr_year))\n",
    "\n",
    "    # reorder columns\n",
    "    prcp_tavg_pivoted = prcp_tavg_pivoted[['Station', 'Date', 'TAVG', 'PRCP']]\n",
    "\n",
    "    # merge PRCP and TAVG table with station dataframe\n",
    "    df_merged = prcp_tavg_pivoted.merge(stations,\n",
    "                                        left_on='Station',\n",
    "                                        right_on='Station',\n",
    "                                        how='left')\n",
    "\n",
    "    # change Date column to datetime type\n",
    "    df_merged['Date'] = pd.to_datetime(df_merged['Date'], format='%Y%m%d')\n",
    "\n",
    "    # change other numerical columns to float\n",
    "    for c in ['Latitude', 'Longitude', 'TAVG', 'PRCP', 'Elevation']:\n",
    "        df_merged[c] = df_merged[c].astype(float)\n",
    "        \n",
    "    print('Finished merge with station and type conversion for {}'\\\n",
    "          .format(curr_year))\n",
    "\n",
    "#     display(df_merged)\n",
    "\n",
    "    # save to CSV\n",
    "    print('Started save for {}'.format(curr_year))\n",
    "    df_merged.to_csv('data_ghcn/daily_global_weather_{}.csv'.format(curr_year))\n",
    "    print('Finished saving CSV to ./data_ghcn/daily_global_weather_{}.csv'\\\n",
    "          .format(curr_year))\n",
    "    print('Finished processing for {}'.format(curr_year))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
